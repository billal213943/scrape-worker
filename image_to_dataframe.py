#!/usr/bin/env python3
"""
Script d'extraction de tableaux FlashBack FA utilisant OpenAI GPT-4 Vision
Analyse directement toutes les images de tableaux et retourne des DataFrames structur√©s
"""

import pandas as pd
import openai
from openai import OpenAI
import base64
import json
import os
import sys
from pathlib import Path
import logging
from typing import List, Dict, Optional, Tuple
import re
from PIL import Image
import io
import shutil

def load_env_file():
    """Charge le fichier .env s'il existe"""
    env_file = Path('.env')
    if env_file.exists():
        try:
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key] = value
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du chargement du fichier .env: {e}")

# Charger les variables d'environnement depuis .env
load_env_file()

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UniversalTableExtractorAI:
    def __init__(self, images_dir: str = "flashback_images", api_key: Optional[str] = None):
        """
        Extracteur universel de tableaux FlashBack FA utilisant GPT-4 Vision
        
        Args:
            images_dir: R√©pertoire contenant les images √† analyser
            api_key: Cl√© API OpenAI (optionnel, peut √™tre d√©finie via variable d'environnement)
        """
        self.images_dir = Path(images_dir)
        
        # Configuration OpenAI
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            logger.error("‚ùå Cl√© API OpenAI non trouv√©e!")
            logger.info("üí° D√©finissez votre cl√© API avec: export OPENAI_API_KEY=your_api_key")
            logger.info("üí° Ou passez-la en param√®tre: UniversalTableExtractorAI(api_key='your_key')")
            sys.exit(1)
        
        self.client = OpenAI(api_key=self.api_key)
        
        # Stockage des DataFrames par type
        self.dataframes = {}

    def encode_image_to_base64(self, image_path: Path) -> str:
        """Encode une image en base64 pour l'API OpenAI"""
        try:
            # Ouvrir et redimensionner l'image si n√©cessaire
            with Image.open(image_path) as img:
                # Convertir en RGB si n√©cessaire
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Redimensionner si trop grande (limite OpenAI: 20MB, recommand√©: 2048x2048)
                max_size = 2048
                if max(img.width, img.height) > max_size:
                    ratio = max_size / max(img.width, img.height)
                    new_width = int(img.width * ratio)
                    new_height = int(img.height * ratio)
                    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                
                # Encoder en base64
                buffer = io.BytesIO()
                img.save(buffer, format='JPEG', quality=85)
                buffer.seek(0)
                return base64.b64encode(buffer.read()).decode('utf-8')
                
        except Exception as e:
            logger.error(f"Erreur lors de l'encodage de {image_path}: {e}")
            return None

    def analyze_all_tables_with_vision(self, image_path: Path) -> Tuple[str, List[Dict]]:
        """Analyse tous les tableaux dans une image avec GPT-4 Vision"""
        try:
            logger.info(f"ü§ñ Analyse IA compl√®te de {image_path.name}")
            
            # Encoder l'image
            base64_image = self.encode_image_to_base64(image_path)
            if not base64_image:
                return None, []
            
            # Prompt universel pour d√©tecter tous les types de tableaux
            system_prompt = """Tu es un expert en analyse de tableaux pour le serveur de jeu FlashBack FA.

            Ton r√¥le est d'identifier et extraire TOUS les tableaux visibles dans l'image, quel que soit leur type (armes, v√©hicules, objets, immobilier, emplois, etc.).

            INSTRUCTIONS:
            1. Examine TOUTE l'image pour d√©tecter tous les tableaux de donn√©es
            2. Pour chaque tableau trouv√©, d√©termine son TYPE (ex: "armes", "v√©hicules", "objets", "immobilier", "emplois")  
            3. Extrait TOUTES les donn√©es de chaque tableau
            4. Conserve la structure et les colonnes exactes de chaque tableau
            5. Les symboles ‚úì/‚úó ou coches/croix = "autoriser"/"interdit"
            6. Retourne un JSON avec la structure suivante:

            {
              "table_type": "nom_du_type_de_tableau",
              "data": [
                {
                  "colonne1": "valeur1",
                  "colonne2": "valeur2",
                  ...
                }
              ]
            }

            Si plusieurs tableaux sont d√©tect√©s, retourne une liste de ces objets.
            Si aucun tableau d√©tect√©, retourne une liste vide []."""
            
            user_prompt = """Analyse cette image FlashBack FA et extrait TOUS les tableaux visibles.

            Pour chaque tableau trouv√©:
            1. Identifie son type (armes, v√©hicules, objets, etc.)
            2. Extrait toutes les donn√©es ligne par ligne
            3. Conserve tous les noms de colonnes exactement comme dans l'image
            4. Convertis les symboles ‚úì/‚úó en "autoriser"/"interdit"

            Retourne UNIQUEMENT un JSON valide avec cette structure:
            [
              {
                "table_type": "type_du_tableau",
                "data": [
                  {"colonne1": "valeur1", "colonne2": "valeur2", ...}
                ]
              }
            ]

            N'ajoute AUCUN texte avant ou apr√®s le JSON."""
            
            # Appel √† l'API OpenAI GPT-4 Vision
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": user_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4000,
                temperature=0.1
            )
            
            # Extraire la r√©ponse
            content = response.choices[0].message.content.strip()
            logger.info(f"ü§ñ R√©ponse IA brute: {content[:200]}...")
            
            # Parser le JSON
            try:
                tables = json.loads(content)
                if isinstance(tables, list):
                    logger.info(f"‚úÖ {len(tables)} tableaux d√©tect√©s par l'IA dans {image_path.name}")
                    return None, tables # Return None for table_type as it's not directly used here
                else:
                    logger.warning(f"‚ö†Ô∏è R√©ponse IA non valide pour {image_path.name}: pas une liste")
                    return None, []
                    
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå Erreur JSON pour {image_path.name}: {e}")
                logger.error(f"Contenu re√ßu: {content}")
                
                # Tentative de nettoyage du JSON
                cleaned_content = self.clean_json_response(content)
                if cleaned_content:
                    try:
                        tables = json.loads(cleaned_content)
                        if isinstance(tables, list):
                            logger.info(f"‚úÖ JSON nettoy√©: {len(tables)} tableaux d√©tect√©s")
                            return None, tables
                    except:
                        pass
                
                return None, []
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse IA de {image_path}: {e}")
            return None, []

    def clean_json_response(self, content: str) -> Optional[str]:
        """Nettoie la r√©ponse IA pour extraire le JSON valide"""
        try:
            # Chercher le JSON entre crochets (liste)
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            if json_match:
                return json_match.group(0)
            
            # Chercher le JSON entre accolades (objet unique)
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return f"[{json_match.group(0)}]"
            
            return None
            
        except Exception as e:
            logger.error(f"Erreur lors du nettoyage JSON: {e}")
            return None

    def normalize_table_data(self, table_data: List[Dict], table_type: str) -> List[Dict]:
        """Normalise les donn√©es d'un tableau selon son type"""
        normalized_data = []
        
        for row in table_data:
            normalized_row = {}
            
            # Normaliser chaque champ
            for key, value in row.items():
                # Nettoyer la cl√©
                clean_key = str(key).strip()
                clean_value = str(value).strip()
                
                # Normaliser les autorisations
                if any(auth_word in clean_key.lower() for auth_word in ['ind√©pendant', 'pf offi', 'gang', 'orga', 'autorisation']):
                    if any(symbol in clean_value.lower() for symbol in ['‚úì', 'oui', 'yes', 'autoris', 'allow', 'permit']):
                        clean_value = 'autoriser'
                    elif any(symbol in clean_value.lower() for symbol in ['‚úó', '‚ùå', 'non', 'no', 'interdit', 'forbid', 'deny']):
                        clean_value = 'interdit'
                
                # Normaliser les prix
                if any(price_word in clean_key.lower() for price_word in ['prix', 'price', 'co√ªt', 'cost', 'revente']):
                    if 'interdit' in clean_value.lower():
                        clean_value = 'INTERDIT'
                    else:
                        price_match = re.search(r'(\d+)', clean_value.replace(' ', '').replace("'", ''))
                        if price_match:
                            price_num = price_match.group(1)
                            if len(price_num) <= 3:
                                clean_value = f"{price_num} 000$"
                            else:
                                clean_value = f"{price_num}$"
                
                # Normaliser les quantit√©s/munitions
                if any(qty_word in clean_key.lower() for qty_word in ['munitions', 'quantit√©', 'max', 'stock']):
                    qty_match = re.search(r'(\d+)', clean_value)
                    if qty_match:
                        clean_value = qty_match.group(1)
                
                normalized_row[clean_key] = clean_value
            
            if normalized_row:  # Ajouter seulement si on a des donn√©es
                normalized_data.append(normalized_row)
        
        return normalized_data

    def process_all_images(self) -> Dict[str, pd.DataFrame]:
        """Traite toutes les images et retourne UN DataFrame par image avec organisation parfaite"""
        logger.info(f"ü§ñ Analyse IA - UN dataset par image dans {self.images_dir}")
        
        dataframes_by_image = {}
        processed_count = 0
        
        # Traiter chaque image individuellement
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.gif', '*.bmp', '*.webp']
        image_files = []
        
        # Collecter toutes les images et les trier
        for ext in image_extensions:
            image_files.extend(self.images_dir.glob(ext))
        
        # Trier les images par nom pour un ordre coh√©rent
        image_files.sort(key=lambda x: x.name)
        
        for image_index, image_path in enumerate(image_files, 1):
            logger.info(f"üì∑ Analyse IA de {image_path.name} ({image_index}/{len(image_files)})")
            
            # Analyser avec GPT-4 Vision
            table_type, tables = self.analyze_all_tables_with_vision(image_path)
            
            if tables:
                # Collecter toutes les donn√©es de cette image
                all_image_data = []
                
                for table_index, table in enumerate(tables):
                    if isinstance(table, dict):
                        table_data = table.get('data', [])
                        detected_type = table.get('table_type', 'tableau').lower()
                    else:
                        # Si c'est directement une liste
                        table_data = table if isinstance(table, list) else []
                        detected_type = 'tableau'
                    
                    if table_data:
                        # Normaliser les donn√©es de ce tableau
                        normalized_data = self.normalize_table_data(table_data, detected_type)
                        
                        if normalized_data:
                            # Ajouter chaque √©l√©ment avec m√©tadonn√©es
                            for item in normalized_data:
                                # Nettoyer l'item - garder seulement les colonnes utiles
                                clean_item = {}
                                
                                # Garder seulement les colonnes qui ont des valeurs
                                for key, value in item.items():
                                    if value and str(value).strip():
                                        clean_item[key] = str(value).strip()
                                
                                # Ajouter les m√©tadonn√©es si l'item a du contenu
                                if clean_item:
                                    clean_item['Table_Type'] = detected_type.capitalize()
                                    clean_item['Source_Image'] = image_path.name
                                    all_image_data.append(clean_item)
                            
                            logger.info(f"  ‚úÖ Tableau '{detected_type}': {len(normalized_data)} √©l√©ments")
                
                # Cr√©er un DataFrame pour cette image si on a des donn√©es
                if all_image_data:
                    # Nom propre et organis√© pour ce dataset
                    dataset_name = f"image_{image_index:02d}"
                    
                    # Supprimer les doublons dans cette image
                    unique_items = []
                    seen_items = set()
                    
                    for item in all_image_data:
                        # Utiliser les premi√®res valeurs significatives comme cl√© unique
                        key_values = []
                        for key, value in item.items():
                            if key not in ['Table_Type', 'Source_Image'] and value and str(value).strip():
                                key_values.append(str(value).lower().strip())
                                if len(key_values) >= 2:  # Utiliser 2 valeurs pour la cl√©
                                    break
                        
                        item_key = '|'.join(key_values) if key_values else str(len(unique_items))
                        
                        if item_key not in seen_items:
                            unique_items.append(item)
                            seen_items.add(item_key)
                    
                    if unique_items:
                        # Cr√©er le DataFrame pour cette image
                        df = pd.DataFrame(unique_items)
                        
                        # R√©organiser les colonnes : donn√©es importantes d'abord, m√©tadonn√©es √† la fin
                        priority_columns = []
                        regular_columns = []
                        meta_columns = []
                        
                        for col in df.columns:
                            if col in ['Table_Type', 'Source_Image']:
                                meta_columns.append(col)
                            elif any(priority in col.upper() for priority in ['ARME', 'NOM', 'OBJET', 'VEHICULE']):
                                priority_columns.append(col)
                            else:
                                regular_columns.append(col)
                        
                        # Nouvelle organisation des colonnes
                        new_column_order = priority_columns + regular_columns + meta_columns
                        df = df[new_column_order]
                        
                        dataframes_by_image[dataset_name] = df
                        logger.info(f"üéØ Dataset cr√©√© '{dataset_name}': {len(df)} √©l√©ments uniques")
                
            processed_count += 1
        
        if not dataframes_by_image:
            logger.warning("‚ùå Aucun dataset cr√©√© - aucun tableau d√©tect√©")
        else:
            logger.info(f"üéâ {len(dataframes_by_image)} datasets cr√©√©s (1 par image):")
            for name, df in dataframes_by_image.items():
                logger.info(f"   üìä {name}: {len(df)} √©l√©ments")
        
        return dataframes_by_image

    def export_all_dataframes(self, dataframes: Dict[str, pd.DataFrame]):
        """Exporte tous les DataFrames en code Python avec nommage parfait - 1 dataset par image"""
        try:
            # Cr√©er le dossier de sortie
            output_dir = Path("flashback_dataframes")
            output_dir.mkdir(exist_ok=True)
            
            timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            
            for dataset_name, df in dataframes.items():
                if not df.empty:
                    # Nom de fichier propre et organis√©
                    clean_dataset_name = f"dataset_{dataset_name}"
                    
                    # Export Python code avec nommage parfait
                    py_file = output_dir / f"flashback_{clean_dataset_name}_data.py"
                    with open(py_file, 'w', encoding='utf-8') as f:
                        f.write("import pandas as pd\n\n")
                        f.write(f"# Dataset FlashBack FA - {dataset_name}\n")
                        f.write(f"# Extrait par IA GPT-4 Vision depuis l'image correspondante\n")
                        f.write(f"# D√âTECTION AUTOMATIQUE PAR INTELLIGENCE ARTIFICIELLE\n")
                        f.write(f"# Timestamp: {timestamp}\n")
                        f.write(f"# Total √©l√©ments: {len(df)}\n\n")
                        
                        # Variables avec noms propres et coh√©rents
                        var_name = f"{clean_dataset_name}_data"
                        df_name = f"{clean_dataset_name}_df"
                        
                        f.write(f"{var_name} = {df_name} = pd.DataFrame([\n")
                        
                        for _, row in df.iterrows():
                            f.write("    {")
                            items = []
                            for col in df.columns:
                                value = row[col] if pd.notna(row[col]) else ""
                                # √âchapper les guillemets dans les valeurs
                                escaped_value = str(value).replace('"', '\\"')
                                items.append(f'"{col}": "{escaped_value}"')
                            f.write(", ".join(items))
                            f.write("},\n")
                        
                        f.write("])\n\n")
                        
                        # Ajouter des exemples d'utilisation
                        f.write(f"# Utilisation:\n")
                        f.write(f"# from flashback_dataframes.flashback_{clean_dataset_name}_data import {var_name}, {df_name}\n")
                        f.write(f"# print({df_name}.head())\n")
                        f.write(f"# print(f'{{len({df_name})}} √©l√©ments dans ce dataset')\n\n")
                        
                        # Statistiques du dataset
                        f.write(f"# Statistiques du dataset:\n")
                        f.write(f"# - Nombre d'√©l√©ments: {len(df)}\n")
                        f.write(f"# - Colonnes: {list(df.columns)}\n")
                        
                        # Types de tableaux d√©tect√©s
                        if 'Table_Type' in df.columns:
                            table_types = df['Table_Type'].value_counts().to_dict()
                            f.write(f"# - Types d√©tect√©s: {dict(table_types)}\n")
                    
                    logger.info(f"‚úÖ '{dataset_name}' export√© vers {py_file}")
                    logger.info(f"   üìù Variables: {var_name}, {df_name}")
                    logger.info(f"   üìä {len(df)} √©l√©ments dans ce dataset")
            
            logger.info(f"üìÅ Tous les datasets export√©s dans: {output_dir.absolute()}")
            logger.info(f"üéØ Organisation: 1 fichier Python = 1 image analys√©e")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'export: {e}")

def clean_output_directory(output_dir: str = "flashback_dataframes"):
    """Nettoie compl√®tement le dossier de sortie avant la nouvelle extraction"""
    output_path = Path(output_dir)
    
    if output_path.exists():
        try:
            # Supprimer tout le dossier et son contenu
            shutil.rmtree(output_path)
            print(f"üóëÔ∏è Dossier {output_dir}/ vid√© (anciens datasets supprim√©s)")
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de vider {output_dir}/: {e}")
    
    # Recr√©er le dossier vide
    try:
        output_path.mkdir(exist_ok=True)
        print(f"üìÅ Dossier {output_dir}/ recr√©√© (pr√™t pour nouveaux datasets)")
    except Exception as e:
        print(f"‚ùå Impossible de cr√©er {output_dir}/: {e}")

def main():
    """Fonction principale"""
    print("üéÆ FLASHBACK FA - EXTRACTEUR IA UNIVERSEL")
    print("ü§ñ ANALYSE PAR INTELLIGENCE ARTIFICIELLE GPT-4 VISION")
    print("üìä EXTRACTION DE TOUS LES TABLEAUX (armes, v√©hicules, objets, etc.)")
    print("=" * 70)
    
    # Nettoyer le dossier de sortie avant de commencer
    clean_output_directory()
    
    # V√©rifier la cl√© API
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ùå Cl√© API OpenAI non trouv√©e!")
        print()
        print("üîß Solutions:")
        print("   1. D√©finir la variable d'environnement: export OPENAI_API_KEY=your_api_key")
        print("   2. Ou cr√©er un fichier .env avec: OPENAI_API_KEY=your_api_key")
        print()
        print("üí° Obtenez votre cl√© API sur: https://platform.openai.com/api-keys")
        return
    
    print(f"‚úÖ Cl√© API OpenAI configur√©e (***{api_key[-6:]})")
    
    # Cr√©er l'extracteur IA
    ai_extractor = UniversalTableExtractorAI("flashback_images")
    
    # V√©rifier le dossier d'images
    if not ai_extractor.images_dir.exists():
        print(f"‚ùå Dossier {ai_extractor.images_dir} non trouv√©!")
        print("   Lancez d'abord le scraper pour t√©l√©charger les images.")
        return
    
    # Compter les images
    image_count = sum(1 for ext in ['*.jpg', '*.jpeg', '*.png', '*.gif', '*.bmp', '*.webp'] 
                     for _ in ai_extractor.images_dir.glob(ext))
    
    if image_count == 0:
        print(f"‚ùå Aucune image trouv√©e dans {ai_extractor.images_dir}")
        return
    
    print(f"üì∑ {image_count} images trouv√©es pour analyse IA")
    
    # Traiter les images avec l'IA
    print("\nü§ñ Analyse par intelligence artificielle...")
    print("üîç GPT-4 Vision va analyser chaque image et extraire TOUS les tableaux")
    print("üìä D√©tection automatique: armes, v√©hicules, objets, immobilier, emplois, etc.")
    dataframes_by_image = ai_extractor.process_all_images()
    
    if dataframes_by_image:
        total_elements = sum(len(df) for df in dataframes_by_image.values())
        print(f"\nüìä {total_elements} √©l√©ments d√©tect√©s par l'IA dans {len(dataframes_by_image)} images!")
        
        print("\nüî§ Aper√ßu des donn√©es extraites par IA:")
        for name, df in dataframes_by_image.items():
            print(f"\n--- {name} ({len(df)} √©l√©ments) ---")
            print(df.head(3))  # Afficher 3 premi√®res lignes
        
        # Exporter au format demand√©
        ai_extractor.export_all_dataframes(dataframes_by_image)
        print("\n‚úÖ Donn√©es IA export√©es vers le dossier flashback_dataframes/")
        
        # Statistiques d√©taill√©es
        print(f"\nüìà Statistiques compl√®tes:")
        for name, df in dataframes_by_image.items():
            print(f"   üìã {name}: {len(df)} √©l√©ments")
            # Afficher les colonnes principales
            main_columns = [col for col in df.columns if not col.startswith('Source_') and col != 'Table_Type']
            print(f"      Colonnes: {', '.join(main_columns[:5])}" + ("..." if len(main_columns) > 5 else ""))
        
        print(f"\nüéØ TOTAL: {total_elements} √©l√©ments extraits automatiquement par l'IA!")
        print("üéÆ Organisation parfaite: 1 dataset par image analys√©e!")
        print("üìÅ Fichiers g√©n√©r√©s dans flashback_dataframes/ :")
        for name, df in dataframes_by_image.items():
            print(f"   üìÑ flashback_dataset_{name}_data.py ({len(df)} √©l√©ments)")
        
    else:
        print("‚ùå Aucun tableau d√©tect√© par l'IA.")
        print("   V√©rifiez que les images contiennent des tableaux de donn√©es lisibles.")
        print("   L'IA recherche: armes, v√©hicules, objets, immobilier, emplois, etc.")

if __name__ == "__main__":
    main() 